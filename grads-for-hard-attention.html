<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
<link rel = 'stylesheet' href = 'https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.1/css/materialize.min.css'>
<script src = 'https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.1/js/materialize.min.js'></script>
<link href = 'https://fonts.googleapis.com/icon?family=Material+Icons' rel = 'stylesheet'>

<link href="https://fonts.googleapis.com/css2?family=DM+Mono:wght@300;400;500&family=DM+Sans&display=swap" rel="stylesheet">

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>

<link href="https://fonts.googleapis.com/css2?family=DM+Serif+Text&display=swap" rel="stylesheet">

<style>
    #banner-title {
        font-family:"europa";
    }

    body {
        font-family: "DM Mono"
    }

    a {
        -webkit-transition: all .5s ease;
        -moz-transitition: all .5s ease;
        -o-transition: all .5s ease;
        transition: all .5s ease;

        color: rgb(10, 100, 240);
        border-bottom-color: rgb(240, 240, 240);
        border-bottom-style: solid;
    }

    a:hover {
        border-bottom-color: rgb(50, 100, 240);
    }

    .eq {
        width: 700px;
        text-align:center;
        margin-top:20px;

    }

    .prettyprint {
        font-size:12px;
    }

    .button-a{
         transition:0.2s all ease;
        -webkit-transition:0.2s all ease;
        -moz-transition:all 0.2s ease;
        -o-transition:all 0.2s ease;

        background-color: rgba(0, 0, 0, 0);
        color:white
    }

    .button-a:hover{
        background-color: rgba(255, 255, 255, .2);
        color:white;
    }

    .header {
        font-size: 25px;
        margin-bottom: 10px;
        
    }

    #content {
        margin-left:200px;
        margin-top:50px;
        font-family: "Georgia", "Segoe UI", "Lato";
        font-size: 20px;
        line-height:1.7em;
        font-weight: 300;
    }

    .text {
        width: 700px;
    }

    .img {
        text-align: center;
        width: 250px; 
        position: absolute;
        top: 0;
        left: 775px;
    }

    .image {
        width: 100%;
    }


    @media only screen and (max-width: 1400px) {
        #content {
            margin-left: 10rem;
            font-size: 19px;
        }

        .text {
            width: 50vw;
            min-width: 500px;
        }

        .img {
            left: 55vw;
        }
    }

    @media only screen and (max-width: 1000px) {
        #content {
            margin-left: 10vw;
            width: 80vw;

            font-size: 18px;
        }

        .text {
            min-width: initial;
            width: 80vw;
            font-size: 18px;
        }

        .img {
            width: 80vw;
            text-align: center;
            position: initial;
        }

        .image {
            margin-top: 30px;
            width: 80%;
        }

        .eq {
            width: 100%;
        }
    }

   
</style>

<head><meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>

<html>
    <body style = ''>

        <div id = 'content' class = '' style = ''>
            <div style = 'font-size: 40px; line-height: 50px; margin-bottom: 20px; margin-top: 100px' class = 'header text'>Pseudo-gradients for hard attention</div>
            <div style = 'font-size: 20px; margin-bottom: 100px; font-family:"DM Mono"; color: gray' class = ' text'>April 21, 2020 · <a href = 'https://github.com/nathan-yan/fractals' style = 'margin-bottom: 100px'>Link to repo</a></div>

            <div style = 'line-height:35px' class = 'header text'>
                Soft attention is the more common route for attention because it can be trained using gradient descent, rather than slower and more fragile methods like policy gradients. What if we could produce useful gradients from a hard attention mechanism, so it can be trained using SGD?
            <br/><br/>

            </div>
            <div class = 'header'>
                Soft and hard attention
            </div>

            
            <div style = 'position: relative'>
                <div class = 'text'>
                    Attention in general is the ability for a network or model to select what it wants to process. This is really important in making models more interpretable, and has empirically shown to be extremely effective in improving model performance. In translation tasks, for example, it may be useful for the model to attend to different parts of the sentence for different words. The token <code>it</code> might refer to a subject mentioned previously in the sentence, so the network may attend somewhere in the past to form its translation.
                </div>
            
                <div class = 'img'>
                    <img src = './assets/attention.PNG' class = 'image'>
                    <br/>
                    <div style = ' font-size: 15px; text-align:justify; line-height: 17px; margin-top: 10px'>A classic example of attention for translation. The attention mostly corresponds word-for-word, except for the phrase "European Economic Area". In French, the phrase is written as "zone économique européene"--the words are backward. The alignment mechanism focuses its attention in the opposite order when generating the those three French words.</div>
                </div>
            </div>
            
            <div style = 'position: relative'>
                <div class = 'text'>
                    <br/>
                    There are two types of attention, hard and soft. Hard is the easiest to explain, it simply chooses specific parts of the input to read and completely ignores the others. In an image setting, this might be something like cropping the image so you can focus on a smaller part of it. The rest of the image is essentially discarded. The important thing to note is that hard attention is usually on-off, all-or-nothing. This type of activation function is not differentiable, because it resembles a step function--where the gradient is zero for all \(x\) except for \(x = 0\), where the gradient is undefined. Intuitively, this is like saying we don't know what the cropped image would look like if we adjusted the attention location by a small amount. Since images have discrete pixels, it doesn't make sense to ask what the image would look like if we moved our crop to the left by \(\delta x \ll 1\). Because of this non-differentiability, hard-attention mechanisms have to be trained using RL methods like REINFORCE. The paper <a>Recurrent Models of Visual Attention</a> [Minh et al.,2015] is a landmark example in this type of RL training.
                    <br/>
                    <br/>
                </div>
                
            </div>

            <div style = 'position: relative'>
                <div class = 'text'>
                    Soft attention is when all parts of the input are processed, but different parts may be weighted less than others. A common soft attention mechanism is key-query matching, in which a network generates a series of key vectors that could represent tokens in a sentence, or vectors in some memory matrix, as well as a query vector. This query vector is compared to all the key vectors using some kind of similarity metric, like cosine similarity, these similarities are used to generate an attention vector that can be elementwise multiplied to all the tokens or memory matrix vectors. [add demo?]
                    In the image setting soft attention can be achieved in multiple ways. One particularly cool method is gaussian filters, used in the paper <a>Deep Recurrent Attentive Writer</a> [Gregor et al.,2014]. Gaussian filters allows the network to actually reduce the dimensionality of its input.
                    
                </div>
                <div class = 'img' style = 'top: initial; bottom: 0'>
                    <img src = './assets/draw_filters.PNG' class = 'image' style = 'top: initial; bottom: 0'>
                    <br/>
                    <div style = ' font-size: 15px; text-align: justify; line-height: 17px; margin-top: 10px'>The DRAW model uses gaussian filters, which are centered around the green dots. Each green dot represents a pixel of the resulting image (in this case, the output will be a 3 x 3 patch of the larger image). You can imagine a 2-d gaussian with a variance parameterized by the neural network output centered around each of the dots. This 2-d gaussian represents the weight of each pixel, which you can dot product with the image to get the intensity of the corresponding output pixel.</div>
                </div>
            </div>

            <br/>

            <div style = 'position: relative'>
                <div class = 'text'>
                    Again the important takeaway is that because soft attention actually does process the entire image and involves purely differentiable functions, the whole attention operation is differentiable. This means it can be trained fairly easily with backpropagation. This is especially important for early learning, because soft attention mechanisms attend to the entire input, they receive a useful gradient signal no matter where it is. This may not be the case for hard attention mechanisms unless it happens to get lucky and attend to a salient patch of information. 
                    <br/>
                    <br/>
                    So far it seems like soft attention is the clear winner because of its ability to be trained with backpropagation. However, hard attention still has multiple desirable properties, like increased efficiency and ease of implementation. One can imagine it would be easier it would be to implement cropping rather than generating filter banks for gaussian filters. In addition, since cropping is a very inexpensive operation, it would likely also save some compute for large images. As a result, hard attention still has some desirable properties.
                    <br/><br/>
                    The point of this project was basically to see if I could produce some learning signal for pure hard attention that could be SGD trainable. I say "pure" because <a>Spatial Transformer Networks</a> [Jaderberg.,2010] do something similar with subgradients of the linear interpolation function, because their sampling function actually does interpolate between adjacent pixels. My hard attention mechanism deals purely with integer crops. I also use subgradients of an interpolation function, but since that interpolation function doesn't actually exist I'm calling these "psuedo-gradients". However since Spatial Transformer Networks do actually lay much of the groundwork for this, I'd call this less of a project and more of a tutorial.
                    
                </div>
                
            </div>

            <br/><br/>

            <!--
            <div class = 'header'>
                Backpropagation recap
            </div>

            <div style = 'position: relative'>
                <div class = 'text'>
                    Backpropagation is a way of propagating gradients from a loss to a bunch of leaf variables. These leaf variables are typically the parameters of a neural network, and the generated gradients describe the vector of steepest ascent of the loss w.r.t. the parameters. It is quite easy then to move in the direction of steepest <i>descent</i> by moving the parameters in the opposite direction. More formally, given a loss \(L\) and parameters \([\theta_0, \theta_1, \theta_2 \cdots \theta_n]\), backpropagation computes \([\frac{\partial{l}}{\partial{\theta_0}}, \frac{\partial{l}}{\partial{\theta_1}} \cdots \frac{\partial{l}}{\partial{\theta_n}}]\). The way these gradients are computed is through the chain rule. The rule for gradients in the 2-D case is that the derivative of a function \(f(g(x))\) w.r.t. \(x\) is \(\frac{\text{d}g}{\text{d}x} \cdot \frac{\text{d}f}{\text{d}g}\)
                    <br/><br/>
                    It's pretty easy to extend this to the n-D case. If you have a function \(\textbf{f} : \mathbb{R}^N \mapsto \mathbb{R}^P\) and another function \(\textbf{g} :\mathbb{R}^M \mapsto \mathbb{R}^N\), the gradient of \(\textbf{f}(\textbf{g}(\textbf{v}))\) w.r.t. \(\textbf{v}\) is \((\textbf{J}_{\textbf{f}}^{\top})(\textbf{J}_{\textbf{g}})\), where \(\textbf{J}_{\textbf{f}}^{\top}\) is the transpose of the jacobian of \(\textbf{f}\) w.r.t. \(\textbf{g}\). This looks kinda nasty, and the more intuitive way of thinking about this is that you're calculating the contribution of each input \([v_0, v_1, \cdots v_M]^{\top}\) to each output of \(\textbf{g}\), then calculating the contribution of each output of \(\textbf{g}\) to each output of \(\textbf{f}\). Since each input \(v_m\) affected every output of \(\textbf{g}\), you need to sum up its contributions to each. Formally, this means for any output \(f_p\), its partial derivative w.r.t. some input \(v_m\) is

                    

                </div>

            </div>

            <div class = 'eq'>
                \(\frac{\partial{f_p}}{\partial{v_m}} = \sum_i{
                    \frac{\partial{f_p}}{\partial{g_i}}\frac{\partial{g_i}}{\partial{v_m}}
                }\)
            </div>


            <div>
                    <div class = 'text'>
                        If you do this for every input \(v_m\) and every output \(f_p\) you will get a \(P \times M\) matrix. This matches our vector form above because the jacobian \(\textbf{J}_{\textbf{g}}\) has shape \(N \times M\) and \(\textbf{J}_{\textbf{f}}\) has shape \(N \times P\). Taking the transpose of \(\textbf{J}_{\textbf{f}}\) and matrix multiplying it by \(\textbf{J}_{\textbf{g}}\) gives you a matrix with shape \(P \times M\). Try writing out the vector equation \(\textbf{J}_{\textbf{f}}^{\top}\textbf{J}_{\textbf{g}}\), and you'll notice it does the same thing as the sum form above!

                    </div>

            </div>-->


            <div class = 'header'>
                Gradient Mechanism
            </div>


            <div class = 'text'>
                Given some square crop \(I\) of an image you can also produce a crop one pixel up, down, left and right, which are called \(I^{up}, I^{down}, I^{left}, I^{right}\) respectively. For now let's just pretend we're dealing with a 1-D image, so only \(I^{left}\) and \(I^{right}\) will be considered. For a particular pixel of \(I\) we <i>assume</i> it interpolates linearly between its adjacent pixels. This ends up looking like a leaky RELU function, where the slopes represent whether the adjacent pixels are brighter or dimmer than the center pixel. The derivative of this leaky ReLU like function is the subderivative   
            </div>
            <div class = 'eq'>
                \(z_{t + 1} = z_{t}^2 + c\)
            </div>
            <div>
                into
            </div>
            <div class = 'eq'>
                \(z_{t + 1} = f_{nn}(z_{t}, c)\)
            </div>
            <br/>
            <div style = 'position: relative'>
                <div class = 'text'>
                    where \(f_{nn}\) is a neural network. The neural network I use in the code has two hidden layers each with 10 hidden nodes. The network has two inputs, one for \(z_t\) and one for \(c\). The output of the network is \(z_{t + 1}\). The weights of the network are also complex numbers, with both the real and imaginary components of the weights initialized with a gaussian distribution. The threshold   I used was \(0.8\), and I evaluated the neural network for 30 iterations. On my first attempt, I got the photo on the right:
                </div>
                <div class = 'img'>
                    <img src = './fractals/fractal1_.png' class = 'image'>
                    <br/>
                    <div style = ' font-size: 15px;'>Way cooler than I expected</div>
                </div>
            </div>

            <br/>

            <div style = 'position: relative;'>
                <div class = 'text'>
                    Way cooler than I expected. I then changed the program slightly so that instead of coloring a pixel black or white based on whether or not its value is greater than the threshold, it's colored based on how many iterations it took to exceed the threshold. This gets us an image that less grainy and shows how <i>quickly</i> a point grows unbounded:

                    <br/><br/>

                    The fractal looks different because network weights were initialized at random. I later started recording the seeds (unfortunately I was pretty disorganized while making this, and I don't really know if the seeds correspond to their original images :/ )
                </div>

                <div  class = 'img' style = 'top: inherit; bottom: 0px'>
                    <img src = './fractals/fractal2_.png' class = 'image'>
                    <br/>
                    <div style = 'font-size: 15px;'>Smoothed version</div>
            
                </div>   
            </div>

            <div class = 'text'>
            </div>

            <br/>
            <br/>

            
            <div class = 'header'>
                Specifics
            </div>

            <div class = 'text'>
                I created three variables: \(c, z, \) and a heatmap which I'll call \(h\). \(c\) and \(z\) represent the variables of the recursive function mentioned before. The heatmap is the fractal image and is what is actually plotted. \(c, z, h\) are parameterized by 5 values, which are \(x_{min}, x_{max}, y_{min}, y_{max}, s\). The first four define the lower and upper bounds of the complex plane we'll plot, and \(s\) represents the step, or resolution.

                <div style = 'display: flex; justify-content: center'>
                <pre style = 'width: fit-content'>
import numpy as np
x = np.arange(x_min, x_max, s)
y = np.arange(x_min, x_max, s)

gx, gy = np.meshgrid(x, y)

c = (gx * 1j + gy).flatten()
z = np.zeros(len(c))    
h = np.zeros_like(z)   
                </pre>
                </div>
                Next I define my model using a set of weights and non-linearities. The weights are initialized by sampling small numbers from a zero-mean gaussian distribution for both the real and imaginary component. I created a number of models that produced interesting results, so I'll just show one of them here:
                <div style = 'display: flex; justify-content: center'>

                <pre>
weight1 = 0.2 * randn(2, 10) * 1j + 0.2 * randn(2, 10)
weight2 = 0.35 * randn(10, 10) * 1j + 0.35 * randn(10, 10)
weight3 = 0.45 * randn(10, 1) * 1j + 0.45 * randn(10, 1)

def model(inp):
    fc1 = (inp @ weight1) ** np.sqrt(2)
    fc2 = np.tan((fc1 @ weight2))
    fc3 = fc2 @ weight3

    return fc3
                </pre>
                </div> 

                Since neural networks are basically just matrix multiplication, I don't have to loop through each of the values of \(c\) and \(z\) and compute the recursive function, I can do it all at once! Right now both \(c\) and \(z\) are flattened arrays of some length \(L\). I add an extra dimension to both of them, making them shape \(L \times 1\), then concatenate along the new axis to produce an array of shape \(L \times 2\). Feeding this into the model produces an output of shape \(L \times 1\). To convert this output back into a fully flattened array we just do some simple array slicing. Once the new \(z\) array is calculated, I check whether each value of the array is greater than the threshold 0.8. This whole process is done in a for loop 30 times.

                <div style = 'display: flex; justify-content: center'>

                <pre>
for iteration in range (30):
    c_ = np.expand_dims(c, -1)
    z_ = np.expand_dims(z, -1)

    inp = np.concatenate([c_, z_], axis = -1)

    # compute z{t + 1}
    z = model(inp)[:, 0]

    h += np.absolute(z) > 0.8
                </pre>
                </div>
                At the end \(h\) is a heatmap showing which points went above the threshold the fastest. This can be directly plotted using matplotlib, or any other plotting library.

                 <div style = 'display: flex; justify-content: center'>

                <pre>
import matplotlib.pyplot as plt
plt.imshow(h)
plt.show()
                </pre>
                </div>
            </div>

            <br/><br/>

            <div class = 'header'>
                Gallery
            </div>

                <div class= 'text'>
                Feel free to do whatever you want with these. You can view full resolution images at <a href = "https://github.com/nathan-yan/fractals/tree/master/fractals">the github repo</a> 
                <br/><br/>
            </div>

        </div>

        <div style = 'display: flex; flex-wrap: wrap'>
            <div style = 'width: 80%; margin-left: 10%; align-items:flex-end;'>
                <img src = './fractals/fractal1_.png' style = 'width:48.5%'>
                <img src = './fractals/fractal2_.png' style = 'width:48.5%;'>
                <img src = './fractals/fractal3_.png' style = 'width:48.5%'>
                <img src = './fractals/fractal4_.png' style = 'width: 48.5%'>
                <img src = './fractals/fractal5_.png' style = 'width: 48.5%'>
                <img src = './fractals/fractal6_.png' style = 'width: 48.5%'>
                <img src = './fractals/fractal7_.png' style = 'width: 48.5%'>
                <img src = './fractals/fractal9_.png' style = 'width: 48.5%'>
                <img src = './fractals/fractal10_.png' style = 'width: 48.5%'>
                <img src = './fractals/fractal11_.png' style = 'width: 48.5%'>
                <img src = './fractals/fractal12_.png' style = 'width: 48.5%'>
                <img src = './fractals/fractal13_.png' style = 'width: 48.5%'>
                <img src = './fractals/fractal14_.png' style = 'width: 48.5%'>
                <img src = './fractals/fractal15_.png' style = 'width: 48.5%'>
                <img src = './fractals/fractal16_.png' style = 'width: 48.5%'>
                <img src = './fractals/fractal17_.png' style = 'width: 48.5%'>        
                <img src = './fractals/fractal8_.png' style = 'width: 100%'>

            </div>
        </div>

        <div style = 'height:50px; width:1px'></div>
    </body>
</html>

<script>

$(document).ready(function(){
      $('.parallax').parallax();
    });

check_resize()

window.addEventListener("resize", check_resize);
</script>
